name: Step 3 - Provisioning EFS for Kubeflow

on:
  workflow_dispatch:
    inputs:
      region:
        description: 'AWS Region'
        required: true
        default: 'us-east-1'
      #keypair:
      #  description: 'SSH Key Pair'
      #  required: true

env:
  AWS_REGION : ${{ github.event.inputs.region}} #Change to reflect your Region
  AWS_ACCOUNT_ID: ${{ secrets.AWS_ACCOUNT_ID }}
  CLUSTER_NAME: 'eks-cluster'

# Permission can be added at job level or workflow level
permissions:
      id-token: write   # This is required for requesting the JWT
      contents: read    # This is required for actions/checkout

jobs:
  efs_aws:
    name: Provisioning EFS
    runs-on: ubuntu-latest
    outputs:
      env-name: ${{ steps.env-name.outputs.environment }}
    steps:
      - name: Checkout
        uses: actions/checkout@v3
      - name: Configure AWS credentials
        id: creds
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.IAC_EKS_ACCESS_KEY }}
          aws-secret-access-key: ${{ secrets.IAC_EKS_SECRET_KEY }}
          aws-region: ${{ github.event.inputs.region}}
      - name: Configure environment name
        id: env-name
        env:
          REPO: ${{ github.repository }}
        run: |
          ENVIRONMENT=`echo $REPO | tr "/" "-"`
          echo "Environment name: $ENVIRONMENT"
          echo "environment=$ENVIRONMENT" >> $GITHUB_OUTPUT
      #- name: Allow passwordless sudo
      #  run: echo '${{ secrets.SUDO_PASSWORD }}' | sudo -Sv
      - name: install envsubst
        id: install-envsubst
        run: |
          curl -L https://github.com/a8m/envsubst/releases/download/v1.2.0/envsubst-`uname -s`-`uname -m` -o envsubst
          chmod +x envsubst
          sudo mv envsubst /usr/local/bin
      - name: install eksctl
        id: install-eksctl
        run: |
          # for ARM systems, set ARCH to: `arm64`, `armv6` or `armv7`
          ARCH=amd64
          PLATFORM=$(uname -s)_$ARCH

          curl -sLO "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$PLATFORM.tar.gz"
          tar -xzf eksctl_$PLATFORM.tar.gz -C /tmp && rm eksctl_$PLATFORM.tar.gz
          sudo mv /tmp/eksctl /usr/local/bin
      - name: install kubectl
        id: install-kubectl
        run: |
          curl -O https://s3.us-west-2.amazonaws.com/amazon-eks/1.27.1/2023-04-19/bin/linux/amd64/kubectl
          chmod +x ./kubectl
          mkdir -p $HOME/bin && cp ./kubectl $HOME/bin/kubectl && export PATH=$HOME/bin:$PATH
          kubectl version --short --client
      - name: Health Check Cluster
        id: health-check-eks
        run: |
          aws eks update-kubeconfig --name ${{env.CLUSTER_NAME}} --region ${{env.AWS_REGION}}
          kubectl config get-contexts

          # Ensure cluster has compute
          kubectl get nodes
      - name: Install the EFS CSI driver
        id: install-efs-csi-driver
        run: |
          kubectl apply -k "github.com/kubernetes-sigs/aws-efs-csi-driver/deploy/kubernetes/overlays/stable/?ref=tags/v1.5.4"
          kubectl get csidriver
      - name: create iam service account
        id: create-iam-sa
        run: |
          aws iam create-policy \
            --policy-name AmazonEKS_EFS_CSI_Driver_Policy \
            --policy-document file://config/ack/addons/csi/efs/efs-csi-policy.json
          
          eksctl create iamserviceaccount \
            --name efs-csi-controller-sa \
            --namespace kube-system \
            --cluster ${{env.CLUSTER_NAME}} \
            --attach-policy-arn arn:aws:iam::$AWS_ACCOUNT_ID:policy/AmazonEKS_EFS_CSI_Driver_Policy \
            --approve \
            --override-existing-serviceaccounts \
            --region ${{env.AWS_REGION}}
          
          kubectl describe -n kube-system serviceaccount efs-csi-controller-sa
            - name: Set VPC ID
        id: set-vpc-id
        run: |
          VPC_ID=$(aws eks describe-cluster \
                    --name ${{env.CLUSTER_NAME}} \
                    --query "cluster.resourcesVpcConfig.vpcId" \
                    --output text)
          echo "VPC ID: $VPC_ID"
          echo "VPC_ID=$VPC_ID" >> $GITHUB_ENV
      - name: Set CIDR range
        id: set-CIDR
        run: |
          cidr_range=$(aws ec2 describe-vpcs \
              --vpc-ids ${{env.VPC_ID}} \
              --query "Vpcs[].CidrBlock" \
              --output text \
              --region ${{env.AWS_REGION}})
          
          echo "CIDR_RANGE: $cidr_range"
          echo "CIDR_RANGE=$cidr_range" >> $GITHUB_ENV
      - name: Set Subnet ID for EFS
        id: set-subnet-id
        run: |
          subnet_ids=$(aws eks describe-nodegroup --cluster-name ${{env.CLUSTER_NAME}} --nodegroup-name on-demand --query 'nodegroup.subnets' --output text)
          subnet_id_1=$(echo "$subnet_ids" | awk '{print $1}')
          subnet_id_2=$(echo "$subnet_ids" | awk '{print $2}')
          
          echo "SUBNET_ID_1: $subnet_id_1"
          echo "SUBNET_ID_1=$subnet_id_1" >> $GITHUB_ENV
          
          echo "SUBNET_ID_2: $subnet_id_2"
          echo "SUBNET_ID_2=$subnet_id_2" >> $GITHUB_ENV
      - name: check security group if exists
        id: check-security-group
        run: |
          security_group_id=$(aws ec2 describe-security-groups --filters Name=vpc-id,Values=${{env.VPC_ID}} Name=group-name,Values=MyEfsSecurityGroup --query 'SecurityGroups[0].GroupId' --output text)
          
          if [[ $security_group_id == 'None' ]]; then
            echo "IAM Service Account does not exist."
            echo "::set-output name=exists::false"
          else
            echo "IAM Service Account exists."
            echo "::set-output name=exists::true"
          
            echo "SECURITY_GROUP_ID: $security_group_id"
            echo "SECURITY_GROUP_ID=$security_group_id" >> $GITHUB_ENV
          fi
      - name: Create security group with an inbound
        id: create-sg-inbound
        if: steps.check-security-group.outputs.exists == 'false'
        run: |
          security_group_id=$(aws ec2 create-security-group \
              --group-name MyEfsSecurityGroup \
              --description "My EFS security group" \
              --vpc-id ${{env.VPC_ID}} \
              --output text)
          
          aws ec2 authorize-security-group-ingress \
              --group-id $security_group_id \
              --protocol tcp \
              --port 2049 \
              --cidr ${{env.CIDR_RANGE}}
          
          echo "SECURITY_GROUP_ID: $security_group_id"
          echo "SECURITY_GROUP_ID=$security_group_id" >> $GITHUB_ENV
      - name: Create an Amazon EFS file system
        id: create-aws-efs
        run: |        
          # Create a file system
          file_system_id=$(aws efs create-file-system \
              --region ${{env.AWS_REGION}} \
              --performance-mode generalPurpose \
              --query 'FileSystemId' \
              --output text)
          
          echo "FILE_SYSTEM_ID: $file_system_id"
          echo "FILE_SYSTEM_ID=$file_system_id" >> $GITHUB_ENV
      - name: Mount efs to node
        id: mount-efs
        run: |
          # Add mount targets
          aws efs create-mount-target \
              --file-system-id ${{env.FILE_SYSTEM_ID}} \
              --subnet-id ${{env.SUBNET_ID_1}} \
              --security-groups ${{env.SECURITY_GROUP_ID}}
          aws efs create-mount-target \
              --file-system-id ${{env.FILE_SYSTEM_ID}} \
              --subnet-id ${{env.SUBNET_ID_2}} \
              --security-groups ${{env.SECURITY_GROUP_ID}}
        continue-on-error: true
      - name: Create dynamic provisioning of storage class
        id: create-dynamic-sc
        run: |
          export file_system_id=${{env.FILE_SYSTEM_ID}}
          yq e '.parameters.fileSystemId = env(file_system_id)' -i config/ack/resources/efs/dynamic-provisioning/sc.yaml
          cat config/resources/efs/dynamic-provisioning/sc.yaml
          
          kubectl apply -f config/ack/resources/efs/dynamic-provisioning/sc.yaml
          
          kubectl get sc
      - name: create sample pvc
        id: create-sample-pvc
        run: |
          export PVC_NAMESPACE=kubeflow-user-example-com
          export CLAIM_NAME=efs-claim

          yq e '.metadata.namespace = env(PVC_NAMESPACE)' -i config/ack/resources/efs/dynamic-provisioning/pvc.yaml
          yq e '.metadata.name = env(CLAIM_NAME)' -i config/ack/resources/efs/dynamic-provisioning/pvc.yaml

          kubectl apply -f config/ack/resources/efs/dynamic-provisioning/pvc.yaml
          
          kubectl get pvc -n $PVC_NAMESPACE






