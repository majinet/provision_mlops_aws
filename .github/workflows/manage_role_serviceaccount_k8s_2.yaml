name: Step 2.3 - Manage Role and Service Account for K8s

on:
  workflow_dispatch:
    inputs:
      region:
        description: 'AWS Region'
        required: true
        default: 'us-east-1'
      #keypair:
      #  description: 'SSH Key Pair'
      #  required: true

env:
  AWS_REGION : ${{ github.event.inputs.region}} #Change to reflect your Region
  AWS_ACCOUNT_ID: ${{ secrets.AWS_ACCOUNT_ID }}
  CLUSTER_NAME: 'eks-cluster'
  ACK_K8S_NAMESPACE: "ack-system-k8s"

# Permission can be added at job level or workflow level
permissions:
      id-token: write   # This is required for requesting the JWT
      contents: read    # This is required for actions/checkout

jobs:
  sa_eks_aws:
    name: Manage Role and Service Account for K8s
    runs-on: ubuntu-latest
    outputs:
      env-name: ${{ steps.env-name.outputs.environment }}
    steps:
      - name: Checkout
        uses: actions/checkout@v3
      - name: Checkout awslabs kubeflow-manifests
        uses: actions/checkout@v3
        with:
          repository: awslabs/kubeflow-manifests
          ref: v1.7.0-aws-b1.0.2
          path: kubeflow-manifests
      - name: Configure AWS credentials
        id: creds
        uses: aws-actions/configure-aws-credentials@v2
        with:
          role-to-assume: arn:aws:iam::${{secrets.AWS_ACCOUNT_ID}}:role/iamAdmin
          role-session-name: iamAdmin
          aws-region: ${{ github.event.inputs.region}}
      - name: Configure environment name
        id: env-name
        env:
          REPO: ${{ github.repository }}
        run: |
          ENVIRONMENT=`echo $REPO | tr "/" "-"`
          echo "Environment name: $ENVIRONMENT"
          echo "environment=$ENVIRONMENT" >> $GITHUB_OUTPUT
      #- name: Allow passwordless sudo
      #  run: echo '${{ secrets.SUDO_PASSWORD }}' | sudo -Sv
      - name: install envsubst
        id: install-envsubst
        run: |
          curl -L https://github.com/a8m/envsubst/releases/download/v1.2.0/envsubst-`uname -s`-`uname -m` -o envsubst
          chmod +x envsubst
          sudo mv envsubst /usr/local/bin
      - name: install eksctl
        id: install-eksctl
        run: |
          # for ARM systems, set ARCH to: `arm64`, `armv6` or `armv7`
          ARCH=amd64
          PLATFORM=$(uname -s)_$ARCH

          curl -sLO "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$PLATFORM.tar.gz"
          tar -xzf eksctl_$PLATFORM.tar.gz -C /tmp && rm eksctl_$PLATFORM.tar.gz
          sudo mv /tmp/eksctl /usr/local/bin
      - name: install kubectl
        id: install-kubectl
        run: |
          curl -O https://s3.us-west-2.amazonaws.com/amazon-eks/1.27.1/2023-04-19/bin/linux/amd64/kubectl
          chmod +x ./kubectl
          mkdir -p $HOME/bin && cp ./kubectl $HOME/bin/kubectl && export PATH=$HOME/bin:$PATH
          kubectl version --short --client
      - name: Health Check Cluster
        id: health-check-eks
        run: |
          aws eks update-kubeconfig --name ${{env.CLUSTER_NAME}} --region ${{env.AWS_REGION}}
          kubectl config get-contexts
      - name: set oidc provider
        id: set-oidc-provider
        run: |
          # Update the service name variables as needed
          OIDC_PROVIDER=$(aws eks describe-cluster --name ${{env.CLUSTER_NAME}} --region ${{env.AWS_REGION}} --query "cluster.identity.oidc.issuer" --output text | sed -e "s/^https:\/\///")
          echo "OIDC_PROVIDER: $OIDC_PROVIDER"
          echo "OIDC_PROVIDER=$OIDC_PROVIDER" >> $GITHUB_ENV
      - name: Check if role exists (ack-rds-controller)
        id: check-role-rds
        run: |
          aws iam get-role --role-name ack-rds-controller > /dev/null 2>&1 || echo "::set-output name=exists::false"
      - name: Create an IAM role and policy for service account (rds) - step 4
        id: create-iam-role-rds
        if: steps.check-role-rds.outputs.exists == 'false'
        run: |
          # Update the service name variables as needed
          SERVICE=rds
          AWS_ACCOUNT_ID=${{secrets.AWS_ACCOUNT_ID}}
          ACK_K8S_NAMESPACE=${{env.ACK_K8S_NAMESPACE}}

          ACK_K8S_SERVICE_ACCOUNT_NAME=ack-${SERVICE}-controller
          
          cat config/ack/controller/ack_rds_trust.json | envsubst > ack_rds_trust.json
          cat ack_rds_trust.json

          ACK_CONTROLLER_IAM_ROLE="ack-${SERVICE}-controller"
          ACK_CONTROLLER_IAM_ROLE_DESCRIPTION="IRSA role for ACK ${SERVICE} controller deployment on EKS cluster using Helm charts"
          aws iam create-role --role-name "${ACK_CONTROLLER_IAM_ROLE}" --assume-role-policy-document file://ack_rds_trust.json --description "${ACK_CONTROLLER_IAM_ROLE_DESCRIPTION}"
          
          # Download the recommended managed and inline policies and apply them to the
          # newly created IRSA role
          BASE_URL=https://raw.githubusercontent.com/aws-controllers-k8s/${SERVICE}-controller/main
          POLICY_ARN_URL=${BASE_URL}/config/iam/recommended-policy-arn
          POLICY_ARN_STRINGS="$(wget -qO- ${POLICY_ARN_URL})"
          
          echo "POLICY_ARN_STRINGS: $POLICY_ARN_STRINGS"

          while IFS= read -r POLICY_ARN; do
              echo -n "Attaching $POLICY_ARN ... "
              aws iam attach-role-policy \
                  --role-name "${ACK_CONTROLLER_IAM_ROLE}" \
                  --policy-arn "${POLICY_ARN}"
              echo "ok."
          done <<< "$POLICY_ARN_STRINGS"
      - name: Check if role exists (ack-s3-controller)
        id: check-role-s3
        run: |
          aws iam get-role --role-name ack-s3-controller > /dev/null 2>&1 || echo "::set-output name=exists::false"
      - name: Create an IAM role and policy for service account (s3) - step 4
        id: create-iam-role-s3
        if: steps.check-role-s3.outputs.exists == 'false'
        run: |
          SERVICE=s3
          
          cat config/ack/controller/ack_s3_trust.json | envsubst > ack_s3_trust.json
          cat ack_s3_trust.json

          ACK_CONTROLLER_IAM_ROLE="ack-${SERVICE}-controller"
          ACK_CONTROLLER_IAM_ROLE_DESCRIPTION="IRSA role for ACK ${SERVICE} controller deployment on EKS cluster using Helm charts"
          
          echo "ACK_CONTROLLER_IAM_ROLE: $ACK_CONTROLLER_IAM_ROLE"
          echo "ACK_CONTROLLER_IAM_ROLE_DESCRIPTION: $ACK_CONTROLLER_IAM_ROLE_DESCRIPTION"
          
          aws iam create-role --role-name "${ACK_CONTROLLER_IAM_ROLE}" --assume-role-policy-document file://ack_s3_trust.json --description "${ACK_CONTROLLER_IAM_ROLE_DESCRIPTION}"
          
          # Download the recommended managed and inline policies and apply them to the
          # newly created IRSA role
          BASE_URL=https://raw.githubusercontent.com/aws-controllers-k8s/${SERVICE}-controller/main
          POLICY_ARN_URL=${BASE_URL}/config/iam/recommended-policy-arn
          POLICY_ARN_STRINGS="$(wget -qO- ${POLICY_ARN_URL})"
          
          echo "POLICY_ARN_STRINGS: $POLICY_ARN_STRINGS"

          while IFS= read -r POLICY_ARN; do
              echo -n "Attaching $POLICY_ARN ... "
              aws iam attach-role-policy \
                  --role-name "${ACK_CONTROLLER_IAM_ROLE}" \
                  --policy-arn "${POLICY_ARN}"
              echo "ok."
          done <<< "$POLICY_ARN_STRINGS"
      - name: Check CloudFormation stack existence (kubeflow-secrets-manager-sa)
        id: check-stack-kubeflow-secrets-manager-sa
        run: |
          if aws cloudformation describe-stacks --stack-name "eksctl-eks-cluster-addon-iamserviceaccount-kubeflow-kubeflow-secrets-manager-sa" >/dev/null 2>&1; then
            echo "::set-output name=exists::true"
          else
            echo "::set-output name=exists::false"
          fi
      - name: create iam service account
        id: create-iam-service-account
        if: steps.check-stack-kubeflow-secrets-manager-sa.outputs.exists == 'false'
        run: |
          eksctl create iamserviceaccount  --name kubeflow-secrets-manager-sa  --namespace kubeflow  --cluster ${{env.CLUSTER_NAME}} --attach-policy-arn  arn:aws:iam::aws:policy/AmazonSSMReadOnlyAccess --attach-policy-arn arn:aws:iam::aws:policy/SecretsManagerReadWrite --override-existing-serviceaccounts   --approve --region ${{env.AWS_REGION}}
      - name: Check if role exists (ack-sagemaker-execution-role)
        id: check-role-sagemaker
        run: |
          aws iam get-role --role-name ack-s3-controller > /dev/null 2>&1 || echo "::set-output name=exists::false"
      - name: create sagemaker execution role
        id: create-sagemaker-execution-role
        if: steps.check-role-sagemaker.outputs.exists == 'false'
        run: |
          export SAGEMAKER_EXECUTION_ROLE_NAME=ack-sagemaker-execution-role

          TRUST="{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Service\": \"sagemaker.amazonaws.com\" }, \"Action\": \"sts:AssumeRole\" } ] }"
          aws iam create-role --role-name ${SAGEMAKER_EXECUTION_ROLE_NAME} --assume-role-policy-document "$TRUST"
          aws iam attach-role-policy --role-name ${SAGEMAKER_EXECUTION_ROLE_NAME} --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
          aws iam attach-role-policy --role-name ${SAGEMAKER_EXECUTION_ROLE_NAME} --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess

          SAGEMAKER_EXECUTION_ROLE_ARN=$(aws iam get-role --role-name ${SAGEMAKER_EXECUTION_ROLE_NAME} --output text --query 'Role.Arn')

          echo $SAGEMAKER_EXECUTION_ROLE_ARN
      #- name: check if sa exists (AMP)
      #  id: check-sa-amp
      #  run: |
      #    name=$(kubectl get sa amp-iamproxy-ingest-service-account -n monitoring --output jsonpath='{.metadata.name}')
      #
      #    if [ "$name" = "amp-iamproxy-ingest-service-account" ]; then
      #      echo "AMP sa exists. Skipping next step..."
      #      echo "::set-output name=exists::true"
      #    else
      #      echo "AMP sa does not exist."
      #      echo "::set-output name=exists::false"
      #    fi
      #- name: create iam service account (AMP)
      #  if: steps.check-sa-amp.outputs.exists == 'false'
      #  run: |
      #    export AMP_POLICY_NAME=amp_policy
      #    export AMP_POLICY_ARN=$(aws iam create-policy --policy-name $AMP_POLICY_NAME --policy-document file://kubeflow-manifests/deployments/add-ons/prometheus/AMPIngestPermissionPolicy.json --query 'Policy.Arn' | tr -d '"')
      #
      #    eksctl create iamserviceaccount --name amp-iamproxy-ingest-service-account --namespace monitoring --cluster ${{env.CLUSTER_NAME}} --attach-policy-arn $AMP_POLICY_ARN --override-existing-serviceaccounts --approve --region ${{env.AWS_REGION}}
      - name: Configure the Profile Controller
        run: |
          export PROFILE_CONTROLLER_POLICY_NAME=kubeflow-profile-controller-policy
          
          aws iam create-policy \
            --region ${{env.AWS_REGION}} \
            --policy-name ${PROFILE_CONTROLLER_POLICY_NAME} \
            --policy-document file://kubeflow-manifests/awsconfigs/infra_configs/iam_profile_controller_policy.json
          
          eksctl create iamserviceaccount \
            --cluster=${{env.CLUSTER_NAME}} \
            --name="profiles-controller-service-account" \
            --namespace=kubeflow \
            --attach-policy-arn="arn:aws:iam::${{secrets.AWS_ACCOUNT_ID}}:policy/${PROFILE_CONTROLLER_POLICY_NAME}" \
            --region=${{env.AWS_REGION}} \
            --override-existing-serviceaccounts \
            --approve
      #- name: Check if role exists (testenv-eks-cluster)
      #  id: check-role-testenv
      #  run: |
      #    aws iam get-role --role-name testenv-${{env.CLUSTER_NAME}}-role > /dev/null 2>&1 || echo "::set-output name=exists::false"
      #- name: create iam role for kubeflow profile
      #  if: steps.check-role-testenv.outputs.exists == 'false'
      #  run: |
      #    export PROFILE_NAME=testenv
      #    export PROFILE_CONTROLLER_POLICY_NAME=kubeflow-profile-controller-policy
          
      #    cat config/eks/oidc-trust-policy.json | envsubst  > oidc-trust-policy.json
          
      #    aws iam create-role --role-name $PROFILE_NAME-${{env.CLUSTER_NAME}}-role --assume-role-policy-document file://oidc-trust-policy.json
      #    aws iam attach-role-policy --role-name $PROFILE_NAME-${{env.CLUSTER_NAME}}-role --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess


