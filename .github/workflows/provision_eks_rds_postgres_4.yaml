name: Step 4 - Provisioning Aurora

on:
  workflow_dispatch:
    inputs:
      region:
        description: 'AWS Region'
        required: true
        default: 'us-east-1'
      #keypair:
      #  description: 'SSH Key Pair'
      #  required: true

env:
  AWS_REGION : ${{ github.event.inputs.region}} #Change to reflect your Region
  AWS_ACCOUNT_ID: ${{ secrets.AWS_ACCOUNT_ID }}
  CLUSTER_NAME: 'eks-cluster'

# Permission can be added at job level or workflow level
permissions:
      id-token: write   # This is required for requesting the JWT
      contents: read    # This is required for actions/checkout

jobs:
  eks_aws:
    name: Provisioning Aurora Postgres
    runs-on: ubuntu-latest
    outputs:
      env-name: ${{ steps.env-name.outputs.environment }}
    steps:
      - name: Checkout
        uses: actions/checkout@v3
      - name: Configure AWS credentials
        id: creds
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.IAC_EKS_ACCESS_KEY }}
          aws-secret-access-key: ${{ secrets.IAC_EKS_SECRET_KEY }}
          aws-region: ${{ github.event.inputs.region}}
      - name: Configure environment name
        id: env-name
        env:
          REPO: ${{ github.repository }}
        run: |
          ENVIRONMENT=`echo $REPO | tr "/" "-"`
          echo "Environment name: $ENVIRONMENT"
          echo "environment=$ENVIRONMENT" >> $GITHUB_OUTPUT
      #- name: Allow passwordless sudo
      #  run: echo '${{ secrets.SUDO_PASSWORD }}' | sudo -Sv
      - name: install envsubst
        id: install-envsubst
        run: |
          curl -L https://github.com/a8m/envsubst/releases/download/v1.2.0/envsubst-`uname -s`-`uname -m` -o envsubst
          chmod +x envsubst
          sudo mv envsubst /usr/local/bin
      - name: Health Check Cluster
        id: health-check-eks
        run: |
          aws eks update-kubeconfig --name ${{env.CLUSTER_NAME}} --region ${{env.AWS_REGION}}
          kubectl config get-contexts

          # Ensure cluster has compute
          kubectl get nodes
      - name: Set VPC ID
        id: set-vpc-id
        run: |
          VPC_ID=$(aws cloudformation describe-stacks --stack-name eksctl-eks-cluster-cluster --query 'Stacks[0].Outputs[?OutputKey==`VPC`].OutputValue' --output text)
          echo "VPC ID: $VPC_ID"
          echo "VPC_ID=$VPC_ID" >> $GITHUB_ENV
      - name: Set Eks-Cluster-SG
        id: set-eks-cluster-sg
        run: |
          Eks_Cluster_SG=$(aws cloudformation describe-stacks --stack-name eksctl-eks-cluster-cluster --query 'Stacks[0].Outputs[?OutputKey==`SecurityGroup`].OutputValue' --output text)
          echo "Eks_Cluster_SG: $Eks_Cluster_SG"
          echo "Eks_Cluster_SG=$Eks_Cluster_SG" >> $GITHUB_ENV
      - name: Check CloudFormation stack existence
        id: check-stack
        run: |
          if aws cloudformation describe-stacks --stack-name aws-db-vpc >/dev/null 2>&1; then
            echo "::set-output name=stack-exists::true"
          else
            echo "::set-output name=stack-exists::false"
          fi
      - name: update cloudformation
        id: update-cloudformation
        if: steps.check-stack.outputs.stack-exists == 'false'
        run: |
          cat cloudformation/private-db-vpc-subnet-group.yaml | envsubst  > private-db-vpc-subnet-group.yaml
          cat cloudformation/private-db-vpc-subnet-group.yaml
      - name: Deploy DB VPC Subnet
        uses: aws-actions/aws-cloudformation-github-deploy@v1
        with:
          name: aws-db-vpc
          template: private-db-vpc-subnet-group.yaml
        if: steps.check-stack.outputs.stack-exists == 'false'
      - name: Get Subnet IDs
        id: get-subnet-ids
        run: |
          subnetIds=$(aws cloudformation describe-stacks --stack-name aws-db-vpc --query 'Stacks[0].Outputs[?OutputKey==`DBSubnetIds`].OutputValue' --output text)
          echo "Subnet IDs: $subnetIds"
          #IFS=", " read -r var1 var2 <<< "$subnetIds"
          echo "EKS_SUBNET_IDS=$subnetIds" >> $GITHUB_ENV
      - name: create namespace
        id: create-ns
        run: |
          kubectl create namespace flyte
        continue-on-error: true
      - name: Set up the database networking
        id: setup-db-network
        run: |
          cat config/flyte/db-subnet-groups.yaml | envsubst  > db-subnet-groups.yaml 
          cat db-subnet-groups.yaml
          kubectl apply -f db-subnet-groups.yaml
      - name: Provision an Amazon RDS for PostgreSQL database instance
        id: create-rds-postgres-db
        run: |
          RDS_DB_USERNAME="flyteadmin"

          kubectl create secret generic -n "flyte" flyte-postgres-creds \
            --from-literal=username="${RDS_DB_USERNAME}" \
            --from-literal=password="${{secrets.FLYTEADMIN}}"
          
          RDS_DB_INSTANCE_NAME="flyteadmin"
          RDS_DB_INSTANCE_CLASS="db.t3.medium"
          RDS_DB_STORAGE_SIZE=20
          
          # get RDS_SECURITY_GROUP_ID
          sg_ids=$(aws cloudformation describe-stacks --stack-name aws-db-vpc --query 'Stacks[0].Outputs[?OutputKey==`SecurityGroups`].OutputValue' --output text)
          echo "SecurityGroup IDs: $sg_ids"
          echo "RDS_SECURITY_GROUP_ID=$sg_ids" >> $GITHUB_ENV

          cat config/flyte/flyte-db.yaml | envsubst  > flyte-db.yaml
          kubectl apply -f flyte-db.yaml
      - name: Sleep for 10 mins
        uses: whatnick/wait-action@master
        with:
          time: '600s'
      - name: health check
        id: health-check
        run: |
          kubectl get dbinstance -n "flyte" "flyteadmin" -o jsonpath='{.status.dbInstanceStatus}'
          
          RDS_DB_INSTANCE_HOST=$(kubectl get dbinstance -n "${APP_NAMESPACE}" "${RDS_DB_INSTANCE_NAME}" \
            -o jsonpath='{.status.endpoint.address}'
          )
          RDS_DB_INSTANCE_PORT=$(kubectl get dbinstance -n "${APP_NAMESPACE}" "${RDS_DB_INSTANCE_NAME}" \
            -o jsonpath='{.status.endpoint.port}'
          )
          
          echo "$RDS_DB_INSTANCE_HOST: $RDS_DB_INSTANCE_PORT"